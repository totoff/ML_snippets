{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TestTimeAugmentation.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"4-pN-QXyD0Dv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":649},"outputId":"e8abf2fd-e621-48a4-9581-479786eca0a3","executionInfo":{"status":"ok","timestamp":1565785443227,"user_tz":-120,"elapsed":304858,"user":{"displayName":"Christophe Grihon","photoUrl":"","userId":"17281359019211225799"}}},"source":["# baseline cnn model for the cifar10 problem, repeated evaluation\n","from numpy import mean\n","from numpy import std\n","from keras.datasets.cifar10 import load_data\n","from keras.utils import to_categorical\n","from keras.models import Sequential\n","from keras.layers import Conv2D\n","from keras.layers import MaxPooling2D\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import BatchNormalization\n","\n","# load and return the cifar10 dataset ready for modeling\n","def load_dataset():\n","\t# load dataset\n","\t(trainX, trainY), (testX, testY) = load_data()\n","\t# normalize pixel values\n","\ttrainX = trainX.astype('float32') / 255\n","\ttestX = testX.astype('float32') / 255\n","\t# one hot encode target values\n","\ttrainY = to_categorical(trainY)\n","\ttestY = to_categorical(testY)\n","\treturn trainX, trainY, testX, testY\n","\n","# define the cnn model for the cifar10 dataset\n","def define_model():\n","\t# define model\n","\tmodel = Sequential()\n","\tmodel.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform', input_shape=(32, 32, 3)))\n","\tmodel.add(BatchNormalization())\n","\tmodel.add(MaxPooling2D((2, 2)))\n","\tmodel.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n","\tmodel.add(BatchNormalization())\n","\tmodel.add(MaxPooling2D((2, 2)))\n","\tmodel.add(Flatten())\n","\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n","\tmodel.add(BatchNormalization())\n","\tmodel.add(Dense(10, activation='softmax'))\n","\t# compile model\n","\tmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\treturn model\n","\n","# fit and evaluate a defined model\n","def evaluate_model(model, trainX, trainY, testX, testY):\n","\t# fit model\n","\tmodel.fit(trainX, trainY, epochs=3, batch_size=128, verbose=0)\n","\t# evaluate model\n","\t_, acc = model.evaluate(testX, testY, verbose=0)\n","\treturn acc\n","\n","# repeatedly evaluate model, return distribution of scores\n","def repeated_evaluation(trainX, trainY, testX, testY, repeats=10):\n","\tscores = list()\n","\tfor _ in range(repeats):\n","\t\t# define model\n","\t\tmodel = define_model()\n","\t\t# fit and evaluate model\n","\t\taccuracy = evaluate_model(model, trainX, trainY, testX, testY)\n","\t\t# store score\n","\t\tscores.append(accuracy)\n","\t\tprint('> %.3f' % accuracy)\n","\treturn scores\n","\n","# load dataset\n","trainX, trainY, testX, testY = load_dataset()\n","# evaluate model\n","scores = repeated_evaluation(trainX, trainY, testX, testY)\n","# summarize result\n","print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 12s 0us/step\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0814 12:19:15.775035 140316988467072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","W0814 12:19:15.807031 140316988467072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","W0814 12:19:15.816351 140316988467072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","W0814 12:19:15.863101 140316988467072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","W0814 12:19:15.864532 140316988467072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","W0814 12:19:18.607935 140316988467072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n","\n","W0814 12:19:18.693651 140316988467072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","W0814 12:19:19.029960 140316988467072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","W0814 12:19:19.176332 140316988467072 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stderr"},{"output_type":"stream","text":["> 0.682\n","> 0.697\n","> 0.692\n","> 0.686\n","> 0.648\n","> 0.680\n","> 0.685\n","> 0.645\n","> 0.689\n","> 0.705\n","Accuracy: 0.681 (0.019)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uYm5sUJ_Fe5l","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":215},"outputId":"75ebe867-d098-4dcb-ff38-123d50dc37ad","executionInfo":{"status":"ok","timestamp":1565788194852,"user_tz":-120,"elapsed":2674675,"user":{"displayName":"Christophe Grihon","photoUrl":"","userId":"17281359019211225799"}}},"source":["# cnn model for the cifar10 problem with test-time augmentation\n","import numpy\n","from numpy import argmax\n","from numpy import mean\n","from numpy import std\n","from numpy import expand_dims\n","from sklearn.metrics import accuracy_score\n","from keras.datasets.cifar10 import load_data\n","from keras.utils import to_categorical\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import Sequential\n","from keras.layers import Conv2D\n","from keras.layers import MaxPooling2D\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import BatchNormalization\n","\n","# load and return the cifar10 dataset ready for modeling\n","def load_dataset():\n","\t# load dataset\n","\t(trainX, trainY), (testX, testY) = load_data()\n","\t# normalize pixel values\n","\ttrainX = trainX.astype('float32') / 255\n","\ttestX = testX.astype('float32') / 255\n","\t# one hot encode target values\n","\ttrainY = to_categorical(trainY)\n","\ttestY = to_categorical(testY)\n","\treturn trainX, trainY, testX, testY\n","\n","# define the cnn model for the cifar10 dataset\n","def define_model():\n","\t# define model\n","\tmodel = Sequential()\n","\tmodel.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform', input_shape=(32, 32, 3)))\n","\tmodel.add(BatchNormalization())\n","\tmodel.add(MaxPooling2D((2, 2)))\n","\tmodel.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n","\tmodel.add(BatchNormalization())\n","\tmodel.add(MaxPooling2D((2, 2)))\n","\tmodel.add(Flatten())\n","\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n","\tmodel.add(BatchNormalization())\n","\tmodel.add(Dense(10, activation='softmax'))\n","\t# compile model\n","\tmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\treturn model\n","\n","# make a prediction using test-time augmentation\n","def tta_prediction(datagen, model, image, n_examples):\n","\t# convert image into dataset\n","\tsamples = expand_dims(image, 0)\n","\t# prepare iterator\n","\tit = datagen.flow(samples, batch_size=n_examples)\n","\t# make predictions for each augmented image\n","\tyhats = model.predict_generator(it, steps=n_examples, verbose=0)\n","\t# sum across predictions\n","\tsummed = numpy.sum(yhats, axis=0)\n","\t# argmax across classes\n","\treturn argmax(summed)\n","\n","# evaluate a model on a dataset using test-time augmentation\n","def tta_evaluate_model(model, testX, testY):\n","\t# configure image data augmentation\n","\tdatagen = ImageDataGenerator(horizontal_flip=True)\n","\t# define the number of augmented images to generate per test set image\n","\tn_examples_per_image = 7\n","\tyhats = list()\n","\tfor i in range(len(testX)):\n","\t\t# make augmented prediction\n","\t\tyhat = tta_prediction(datagen, model, testX[i], n_examples_per_image)\n","\t\t# store for evaluation\n","\t\tyhats.append(yhat)\n","\t# calculate accuracy\n","\ttestY_labels = argmax(testY, axis=1)\n","\tacc = accuracy_score(testY_labels, yhats)\n","\treturn acc\n","\n","# fit and evaluate a defined model\n","def evaluate_model(model, trainX, trainY, testX, testY):\n","\t# fit model\n","\tmodel.fit(trainX, trainY, epochs=3, batch_size=128, verbose=0)\n","\t# evaluate model using tta\n","\tacc = tta_evaluate_model(model, testX, testY)\n","\treturn acc\n","\n","# repeatedly evaluate model, return distribution of scores\n","def repeated_evaluation(trainX, trainY, testX, testY, repeats=10):\n","\tscores = list()\n","\tfor _ in range(repeats):\n","\t\t# define model\n","\t\tmodel = define_model()\n","\t\t# fit and evaluate model\n","\t\taccuracy = evaluate_model(model, trainX, trainY, testX, testY)\n","\t\t# store score\n","\t\tscores.append(accuracy)\n","\t\tprint('> %.3f' % accuracy)\n","\treturn scores\n","\n","# load dataset\n","trainX, trainY, testX, testY = load_dataset()\n","# evaluate model\n","scores = repeated_evaluation(trainX, trainY, testX, testY)\n","# summarize result\n","print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["> 0.691\n","> 0.667\n","> 0.694\n","> 0.680\n","> 0.686\n","> 0.646\n","> 0.708\n","> 0.705\n","> 0.676\n","> 0.683\n","Accuracy: 0.683 (0.017)\n"],"name":"stdout"}]}]}